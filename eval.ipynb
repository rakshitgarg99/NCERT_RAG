{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "from bert_score import score as bert_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics functions\n",
    "def safe_divide(numerator, denominator):\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def is_similar(text1, text2, threshold=0.6):\n",
    "    return SequenceMatcher(None, text1, text2).ratio() > threshold\n",
    "\n",
    "def compute_precision_at_k(relevant_at_k):\n",
    "    return safe_divide(sum(relevant_at_k), len(relevant_at_k))\n",
    "\n",
    "def compute_recall_at_k(relevant_at_k, total_relevant):\n",
    "    return safe_divide(sum(relevant_at_k), total_relevant)\n",
    "\n",
    "def compute_mrr(relevant_at_k):\n",
    "    try:\n",
    "        first_relevant_rank = next(i for i, r in enumerate(relevant_at_k, 1) if r) + 1\n",
    "        return 1 / first_relevant_rank\n",
    "    except StopIteration:\n",
    "        return 0\n",
    "\n",
    "def compute_dcg(relevances):\n",
    "    return sum((2**rel - 1) / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "\n",
    "def compute_ndcg(relevant_at_k):\n",
    "    dcg = compute_dcg(relevant_at_k)\n",
    "    idcg = compute_dcg(sorted(relevant_at_k, reverse=True))\n",
    "    return safe_divide(dcg, idcg)\n",
    "\n",
    "def compute_rouge_l(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, candidate)['rougeL'].fmeasure\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    return bleu.compute(predictions=[candidate], references=[[reference]])['bleu']\n",
    "\n",
    "def compute_bert_score(reference, candidate):\n",
    "    _, _, f1 = bert_score([candidate], [reference], lang=\"en\")\n",
    "    return f1.mean().item()\n",
    "\n",
    "def compute_exact_match(reference, candidate):\n",
    "    return int(candidate.strip().lower() == reference.strip().lower())\n",
    "\n",
    "def compute_f1(reference, candidate):\n",
    "    ref_tokens = reference.split()\n",
    "    cand_tokens = candidate.split()\n",
    "    common = Counter(ref_tokens) & Counter(cand_tokens)\n",
    "    num_common = sum(common.values())\n",
    "    \n",
    "    precision = safe_divide(num_common, len(cand_tokens))\n",
    "    recall = safe_divide(num_common, len(ref_tokens))\n",
    "    \n",
    "    return safe_divide(2 * precision * recall, precision + recall)\n",
    "\n",
    "# RAG system setup functions\n",
    "def load_vector_db(vector_db_path):\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vector_db = Chroma(persist_directory=vector_db_path, embedding_function=embeddings)\n",
    "    return vector_db\n",
    "\n",
    "def setup_retriever(vector_db, llm):\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=vector_db.as_retriever(search_kwargs={\"k\": 7})\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "def setup_llm(api_key, model_name):\n",
    "    return ChatGroq(\n",
    "        groq_api_key=api_key,\n",
    "        model=model_name\n",
    "    )\n",
    "\n",
    "def setup_prompt():\n",
    "    template = \"\"\"Answer the question based on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Provide a concise and accurate answer:\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def load_evaluation_dataset(file_path, sample_size=5):\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    df = df.sample(n=sample_size)\n",
    "    return df[['Question', 'Answer']].apply(\n",
    "        lambda row: {\"question\": row['Question'], \"answer\": row['Answer']}, axis=1\n",
    "    ).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation function\n",
    "def evaluate_rag_system(evaluation_dataset, retriever, llm, prompt):\n",
    "    metrics = {\n",
    "        'precision_at_k': [], 'recall_at_k': [], 'mrr': [], 'ndcg': [],\n",
    "        'rouge_l': [], 'bleu': [], 'bert_score': [], 'exact_match': [],\n",
    "        'f1': [], 'response_time': []\n",
    "    }\n",
    "\n",
    "    for sample in tqdm(evaluation_dataset):\n",
    "        question = sample[\"question\"]\n",
    "        reference_answer = sample[\"answer\"]\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        # print(\"hello\")\n",
    "        start_time = time.time()\n",
    "        relevant_documents = retriever.get_relevant_documents(question)\n",
    "        # print(relevant_documents)\n",
    "        retrieval_time = time.time() - start_time\n",
    "\n",
    "        # Generate answer\n",
    "        context = \"\\n\".join([doc.page_content for doc in relevant_documents])\n",
    "        chain = prompt | llm\n",
    "        start_time = time.time()\n",
    "        generated_answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        response_time = retrieval_time + generation_time\n",
    "        \n",
    "        # Extract relevant information\n",
    "        relevant_at_k = [is_similar(doc.page_content, reference_answer) for doc in relevant_documents[:5]]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics['precision_at_k'].append(compute_precision_at_k(relevant_at_k))\n",
    "        metrics['recall_at_k'].append(compute_recall_at_k(relevant_at_k, len(relevant_documents)))\n",
    "        metrics['mrr'].append(compute_mrr(relevant_at_k))\n",
    "        metrics['ndcg'].append(compute_ndcg(relevant_at_k))\n",
    "\n",
    "        generated_answer_text = generated_answer.content if hasattr(generated_answer, 'content') else str(generated_answer)\n",
    "\n",
    "        metrics['rouge_l'].append(compute_rouge_l(reference_answer, generated_answer_text))\n",
    "        metrics['bleu'].append(compute_bleu(reference_answer, generated_answer_text))\n",
    "        metrics['bert_score'].append(compute_bert_score(reference_answer, generated_answer_text))\n",
    "        metrics['exact_match'].append(compute_exact_match(reference_answer, generated_answer_text))\n",
    "        metrics['f1'].append(compute_f1(reference_answer, generated_answer_text))\n",
    "        metrics['response_time'].append(response_time)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def print_evaluation_results(metrics):\n",
    "    for metric, values in metrics.items():\n",
    "        print(f\"Average {metric.replace('_', ' ').title()}: {np.mean(values)}\")\n",
    "    print(f\"Number of samples processed: {len(metrics['precision_at_k'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  5%|▌         | 1/20 [00:16<05:18, 16.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 2/20 [00:33<04:58, 16.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 15%|█▌        | 3/20 [01:27<09:32, 33.67s/it]"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "# Configuration\n",
    "vector_db_path = \"vector_db_eng_hornbill\"\n",
    "groq_api_key = \"\"\n",
    "model_name = \"mixtral-8x7b-32768\"\n",
    "evaluation_dataset_path = \"eng_book.csv\"\n",
    "    \n",
    "# Setup\n",
    "vector_db = load_vector_db(vector_db_path)\n",
    "llm = setup_llm(groq_api_key, model_name)\n",
    "retriever = setup_retriever(vector_db, llm)\n",
    "prompt = setup_prompt()\n",
    "    \n",
    "# Load evaluation dataset\n",
    "evaluation_dataset = load_evaluation_dataset(evaluation_dataset_path, 20)\n",
    "    \n",
    "# Run evaluation\n",
    "metrics = evaluate_rag_system(evaluation_dataset, retriever, llm, prompt)\n",
    "    \n",
    "# Print results\n",
    "print_evaluation_results(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the language spoken in Flanders?',\n",
       "  'answer': 'The French language is spoken in Flanders.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
